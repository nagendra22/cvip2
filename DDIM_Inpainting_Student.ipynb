{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfcb1b40",
   "metadata": {},
   "source": [
    "# DDIM Inpainting on MNIST\n",
    "**Learning:** Build a DDIM pipeline for center-hole inpainting: timestep-conditioned U-Net, DDIM sampling, hole-only PSNR/L1.  \n",
    "**Goal:** Train on masked MNIST, then sample reconstructions and evaluate only inside the mask.  \n",
    "> **Tip:** Use **Google Colab (GPU)** and keep batch size modest to avoid OOM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb8157",
   "metadata": {},
   "source": [
    "## 0. Prerequisites (Colab GPU)\n",
    "- In Colab: **Runtime → Change runtime type → Hardware accelerator → GPU → Save**.\n",
    "- CPU works but training will be **much slower**.\n",
    "- This notebook installs only minimal packages.\n",
    "\n",
    "> If you prefer Kaggle: enable GPU in **Settings → Accelerator → GPU**.\n",
    "\n",
    "**Quick GPU check (run once):**\n",
    "```python\n",
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "if torch.cuda.is_available(): print(torch.cuda.get_device_name(0))\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d5a82",
   "metadata": {},
   "source": [
    "## 1. Install minimal dependencies\n",
    "\n",
    "- **Colab users:** PyTorch is already installed—**don’t reinstall `torch`/`torchvision`**. Install only extras below.  \n",
    "- **Non-Colab users:** Use the pinned install shown after the Colab snippet.\n",
    "\n",
    "**Colab (recommended):**\n",
    "```bash\n",
    "!pip -q install -U tqdm matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6f65b",
   "metadata": {},
   "source": [
    "## 2. Check GPU\n",
    "\n",
    "Run this cell to verify that CUDA is available (Colab GPU recommended):\n",
    "\n",
    "```python\n",
    "import torch, platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "def check_tpu_available():\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        return True, xm.xla_device()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False, \"\"\n",
    "\n",
    "def check_cuda_available():\n",
    "    if torch.cuda.is_available():\n",
    "        return True, torch.cuda.get_device_name(0)\n",
    "    else:\n",
    "        return False, \"\"\n",
    "\n",
    "def check_mps_available():\n",
    "    if torch.backends.mps.is_available():\n",
    "        mps_device = torch.device(\"mps\")\n",
    "        return True, torch.ones(1, device=mps_device)\n",
    "    else:\n",
    "        return False, \"\"\n",
    "\n",
    "tpu_available = check_tpu_available()\n",
    "cuda_available = check_cuda_available()\n",
    "mps_available = check_mps_available()\n",
    "\n",
    "if (tpu_available[0]):\n",
    "    print(\"TPU:\", tpu_available[1])\n",
    "if (cuda_available[0]):\n",
    "    print(\"CUDA:\", cuda_available[1])\n",
    "if (mps_available[0]):\n",
    "    print(\"MPS:\", mps_available[1])\n",
    "\n",
    "if(not(tpu_available[0] or cuda_available[0] or mps_available[0])):\n",
    "    print(\"Running on CPU — training will be much slower.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a63f8f",
   "metadata": {},
   "source": [
    "## 3. Quick Equations (reference)\n",
    "\n",
    "**Forward noising**\n",
    "$$\n",
    "x_t=\\sqrt{\\bar{\\alpha}_t}\\,x_0+\\sqrt{1-\\bar{\\alpha}_t}\\,\\varepsilon,\\qquad \\varepsilon\\sim\\mathcal N(0,I)\n",
    "$$\n",
    "\n",
    "**Cosine schedule — beta\\_t from abar(t)**\n",
    "$$\n",
    "\\bar{\\alpha}(t)=\\cos^2\\!\\left(\\frac{t/T+s}{1+s}\\cdot\\frac{\\pi}{2}\\right)\n",
    "$$\n",
    "$$\n",
    "\\beta_t=\\min\\!\\left(0.999,\\;1-\\frac{\\bar{\\alpha}(t+1)}{\\bar{\\alpha}(t)}\\right)\n",
    "$$\n",
    "\n",
    "**Prediction-space mapping**\n",
    "- **pred = `eps`**\n",
    "$$\n",
    "\\hat\\varepsilon=\\text{out},\\qquad\n",
    "\\hat x_0=\\frac{x_t-\\sqrt{1-\\bar{\\alpha}_t}\\,\\hat\\varepsilon}{\\sqrt{\\bar{\\alpha}_t}}\n",
    "$$\n",
    "\n",
    "- **pred = `x0`**\n",
    "$$\n",
    "\\hat x_0=\\text{out},\\qquad\n",
    "\\hat\\varepsilon=\\frac{x_t-\\sqrt{\\bar{\\alpha}_t}\\,\\hat x_0}{\\sqrt{1-\\bar{\\alpha}_t}}\n",
    "$$\n",
    "\n",
    "- **pred = `v`**\n",
    "$$\n",
    "\\hat x_0=\\sqrt{\\bar{\\alpha}_t}\\,x_t-\\sqrt{1-\\bar{\\alpha}_t}\\,\\text{out},\\qquad\n",
    "\\hat\\varepsilon=\\sqrt{1-\\bar{\\alpha}_t}\\,x_t+\\sqrt{\\bar{\\alpha}_t}\\,\\text{out}\n",
    "$$\n",
    "\n",
    "**DDIM step** (optional noise eta)\n",
    "$$\n",
    "\\sigma_t=\\eta\\cdot\\sqrt{\\frac{1-\\bar{\\alpha}_{t'}}{1-\\bar{\\alpha}_t}\\left(1-\\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t'}}\\right)}\n",
    "$$\n",
    "$$\n",
    "x_{t'}=\\sqrt{\\bar{\\alpha}_{t'}}\\,\\hat x_0+\\sqrt{1-\\bar{\\alpha}_{t'}-\\sigma_t^{2}}\\,\\hat\\varepsilon+\\mathbf{1}_{\\eta>0}\\,\\sigma_t z\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11714c0",
   "metadata": {},
   "source": [
    "## 4. Utilities & Data _(GIVEN — do not modify)_\n",
    "\n",
    "Helper functions used throughout the notebook:\n",
    "- **Repro & device:** `seed_all`, `dev`\n",
    "- **I/O & viz:** `ensure_dir`, `save_grid`, `safe_torch_load`\n",
    "- **Data:** `get_mnist_loader` (MNIST → 32×32, normalized to \\([-1,1]\\))\n",
    "- **Masking:** `make_center_box_mask` (zeros at center box = hole)\n",
    "- **Metrics (hole-only):** `psnr_on_mask`, `l1_on_mask`\n",
    "\n",
    "> Note: `seed_all` sets `cudnn.benchmark=True` for speed (slightly non-deterministic). If you need **strict reproducibility**, temporarily set it to `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa283d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, random\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# Utils\n",
    "# ----------------------------\n",
    "def seed_all(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def dev():\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        return xm.xla_device()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True); return p\n",
    "\n",
    "def save_grid(x, path, nrow=8, rng=(-1,1), dpi=220):\n",
    "    ensure_dir(os.path.dirname(path) or \".\")\n",
    "    grid = vutils.make_grid(x, nrow=nrow, normalize=True, value_range=rng)\n",
    "    plt.figure(figsize=(5,5)); plt.axis(\"off\")\n",
    "    arr = grid.detach().cpu().numpy().transpose(1,2,0)\n",
    "    if arr.shape[2] == 1:\n",
    "        arr = np.repeat(arr, 3, axis=2)\n",
    "    plt.imshow(arr); plt.tight_layout(); plt.savefig(path, dpi=dpi); plt.close()\n",
    "\n",
    "def safe_torch_load(path, map_location=\"cpu\"):\n",
    "    try:\n",
    "        return torch.load(path, map_location=map_location, weights_only=True)  # PyTorch ≥2.4\n",
    "    except TypeError:\n",
    "        return torch.load(path, map_location=map_location)\n",
    "\n",
    "def get_mnist_loader(batch_size=128, num_workers=2):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32,32), antialias=True),\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # [-1,1]\n",
    "    ])\n",
    "    tr = datasets.MNIST('./data', train=True,  transform=tfm, download=True)\n",
    "    te = datasets.MNIST('./data', train=False, transform=tfm, download=True)\n",
    "    train = DataLoader(tr, batch_size, shuffle=True,  num_workers=num_workers, drop_last=True)\n",
    "    test  = DataLoader(te, batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    return train, test, (1,32,32)\n",
    "\n",
    "# ----------------------------\n",
    "# Masks (centered, fixed size)\n",
    "# ----------------------------\n",
    "def make_center_box_mask(B, H, W, box=12):\n",
    "    m = torch.ones(B,1,H,W)\n",
    "    hs, ws = H//2, W//2\n",
    "    h0, h1 = hs - box//2, hs + box//2\n",
    "    w0, w1 = ws - box//2, ws + box//2\n",
    "    m[:,:,h0:h1,w0:w1] = 0.0\n",
    "    return m\n",
    "\n",
    "# ----------------------------\n",
    "# Metrics (hole region only)\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def psnr_on_mask(pred, target, known_mask, eps=1e-8):\n",
    "    hole = (1.0 - known_mask)\n",
    "    N = hole.sum().clamp_min(1.0)\n",
    "    p = (pred+1)/2; t = (target+1)/2\n",
    "    mse = ((p - t)**2 * hole).sum() / N\n",
    "    return float(10.0 * torch.log10(1.0 / (mse + eps)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def l1_on_mask(pred, target, known_mask):\n",
    "    hole = (1.0 - known_mask)\n",
    "    N = hole.sum().clamp_min(1.0)\n",
    "    return float(((pred - target).abs() * hole).sum() / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdcf64",
   "metadata": {},
   "source": [
    "## 5. Model & Diffusion — YOUR IMPLEMENTATIONS (TODO)\n",
    "\n",
    "You’ll implement the core pieces of the model and sampler. Keep interfaces unchanged; only fill the **TODO** parts. Use the **Quick Equations** section above as reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23565edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embed(t: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    ### TODO: implement sinusoidal timestep embedding ϕ(t) ∈ ℝ^{B×dim}.\n",
    "\n",
    "    Let H = ⌊dim/2⌋, and frequencies\n",
    "      f_k = exp( - (log 10000) * k / max(1, H-1) ),  k = 0..H-1.\n",
    "\n",
    "    Define\n",
    "      ϕ(t) = [ sin(t f_0), ..., sin(t f_{H-1}),  cos(t f_0), ..., cos(t f_{H-1}) ].\n",
    "    If dim is odd, append one zero column.\n",
    "\n",
    "    Inputs:\n",
    "      t: (B,) Long/float tensor (will cast to float internally)\n",
    "      dim: embedding size\n",
    "    Output:\n",
    "      (B, dim) on same device as t.\n",
    "    \"\"\"\n",
    "    # ====== Implement here ======\n",
    "    # Half dimension\n",
    "    half_dim = dim // 2\n",
    "    \n",
    "    # Calculate frequencies: f_k = exp( - (log 10000) * k / max(1, H-1) )\n",
    "    # k = 0 ... H-1\n",
    "    emb_scale = math.log(10000) / (half_dim - 1) if half_dim > 1 else 1.0\n",
    "    freqs = torch.exp(-emb_scale * torch.arange(half_dim, device=t.device, dtype=torch.float32))\n",
    "    \n",
    "    # Compute arguments: t * f_k\n",
    "    # t shape: (B,), freqs shape: (H,) -> args shape: (B, H)\n",
    "    args = t.unsqueeze(1).float() * freqs.unsqueeze(0)\n",
    "    \n",
    "    # Cat sin and cos: [sin(args), cos(args)]\n",
    "    embedding = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    \n",
    "    # If dim is odd, pad with one zero column\n",
    "    if dim % 2 == 1:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        \n",
    "    return embedding\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    ### TODO: Residual block with time embedding injection\n",
    "\n",
    "    Given input x ∈ ℝ^{B×C×H×W} and time embedding e ∈ ℝ^{B×E},\n",
    "    compute:\n",
    "      h = x + MLP(e)[:, :, None, None]\n",
    "      h = GN(C) → SiLU → Conv2d(C→C, 3×3, s=1, p=1)\n",
    "      h = Dropout2d(p=dropout)\n",
    "      h = GN(C) → SiLU → Conv2d(C→C, 3×3, s=1, p=1)\n",
    "      out = x + h\n",
    "\n",
    "    Use GroupNorm with 'groups' (cap to divisors of C; at least 1).\n",
    "    \"\"\"\n",
    "    def __init__(self, ch: int, emb: int, dropout: float=0.1, groups: int=8):\n",
    "        super().__init__()\n",
    "        # ====== Implement here ======\n",
    "        # Time embedding projection: MLP(e) -> ch\n",
    "        # We need to project the embedding size 'emb' to the channel size 'ch'\n",
    "        # so we can add it to x.\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb, ch)\n",
    "        )\n",
    "        \n",
    "        # [cite_start]GroupNorm groups: cap to divisors of C; at least 1 [cite: 57, 58]\n",
    "        # We ensure groups divides ch.\n",
    "        safe_groups = groups\n",
    "        while ch % safe_groups != 0 and safe_groups > 1:\n",
    "            safe_groups -= 1\n",
    "            \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(safe_groups, ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, ch, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(safe_groups, ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, ch, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, e: torch.Tensor) -> torch.Tensor:\n",
    "        # ====== Implement here ======\n",
    "        # Note: Spec implies adding time embedding at the start of the branch.\n",
    "        # Project time embedding to match channels\n",
    "        time_emb = self.time_mlp(e) \n",
    "        h = x + time_emb[:, :, None, None]\n",
    "        \n",
    "        # h = GN(C) -> SiLU -> Conv2d\n",
    "        h = self.block1(h)\n",
    "        \n",
    "        # h = Dropout2d\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        # h = GN(C) -> SiLU -> Conv2d\n",
    "        h = self.block2(h)\n",
    "        \n",
    "        # out = x + h\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class SelfAttention2d(nn.Module):\n",
    "    r\"\"\"\n",
    "    ### TODO: Single-head self-attention at spatial resolution (e.g., 8×8).\n",
    "\n",
    "    Normalize with GroupNorm(8, C).\n",
    "    Compute q,k,v via 1×1 convs (C→C).\n",
    "    Shapes:\n",
    "      x: (B,C,H,W)\n",
    "      q: (B,HW,C)   from (B,C,H,W) → (B,C,HW) → (B,HW,C)\n",
    "      k: (B,C,HW)\n",
    "      v: (B,HW,C)\n",
    "      attn = softmax( (q @ k)/√C , dim=-1 )   → (B,HW,HW)\n",
    "      out  = (attn @ v) → (B,HW,C) → (B,C,H,W) → proj 1×1 → residual add.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch: int):\n",
    "        super().__init__()\n",
    "        # ====== Implement here ======\n",
    "        self.ch = ch\n",
    "        self.norm = nn.GroupNorm(8, ch)\n",
    "        self.to_q = nn.Conv2d(ch, ch, 1)\n",
    "        self.to_k = nn.Conv2d(ch, ch, 1)\n",
    "        self.to_v = nn.Conv2d(ch, ch, 1)\n",
    "        self.proj = nn.Conv2d(ch, ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====== Implement here ======\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Normalize\n",
    "        h = self.norm(x)\n",
    "        \n",
    "        # Compute q, k, v via 1x1 convs\n",
    "        q = self.to_q(h)\n",
    "        k = self.to_k(h)\n",
    "        v = self.to_v(h)\n",
    "        \n",
    "        # Reshape for attention: (B, C, H, W) -> (B, C, HW) -> (B, HW, C) (for q)\n",
    "        # k needs to be (B, C, HW) for matmul\n",
    "        q = q.view(B, C, -1).permute(0, 2, 1) # (B, HW, C)\n",
    "        k = k.view(B, C, -1)                 # (B, C, HW)\n",
    "        v = v.view(B, C, -1).permute(0, 2, 1) # (B, HW, C)\n",
    "        \n",
    "        # attn = softmax( (q @ k) / sqrt(C), dim=-1 )\n",
    "        scaling = C ** -0.5\n",
    "        attn_scores = torch.bmm(q, k) * scaling # (B, HW, HW)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # out = attn @ v -> (B, HW, C)\n",
    "        out = torch.bmm(attn_probs, v)\n",
    "        \n",
    "        # Reshape back to (B, C, H, W)\n",
    "        out = out.permute(0, 2, 1).view(B, C, H, W)\n",
    "        \n",
    "        # proj 1x1 -> residual add\n",
    "        out = self.proj(out)\n",
    "        return x + out\n",
    "\n",
    "\n",
    "class UNetDeep(nn.Module):\n",
    "    r\"\"\"\n",
    "    ### TODO: Implement the UNet backbone with time-conditioning and mid attention.\n",
    "\n",
    "    Input channels depend on flags:\n",
    "      - Base: [x_t, m, y] → 3 channels\n",
    "      - + self-conditioning: add x0_sc → +1 channel (total 4)\n",
    "      - + coord_conv: add (coord_x, coord_y) → +2 channels (total 5 or 6)\n",
    "\n",
    "    Spec (assume base=B):\n",
    "      t-MLP: Linear(E→2E) → SiLU → Linear(2E→E)\n",
    "      Down:\n",
    "        inp:  Conv2d(in_ch→B, 3×3,1,1)\n",
    "        rb0:  ResBlock(B,E)\n",
    "        d1 :  Conv2d(B→2B, 4×4,2,1)\n",
    "        rb1:  ResBlock(2B,E)\n",
    "        d2 :  Conv2d(2B→4B, 4×4,2,1)\n",
    "        rb2:  ResBlock(4B,E)\n",
    "      Mid (8×8):\n",
    "        mid1: ResBlock(4B,E)\n",
    "        attn: SelfAttention2d(4B)\n",
    "        mid2: ResBlock(4B,E)\n",
    "      Up:\n",
    "        u1 :  ConvTranspose2d(4B→2B, 4×4,2,1)\n",
    "        red1: Conv2d(2B+2B→2B, 1×1)\n",
    "        rb3:  ResBlock(2B,E)\n",
    "        u2 :  ConvTranspose2d(2B→B, 4×4,2,1)\n",
    "        red2: Conv2d(B+B→B, 1×1)\n",
    "        rb4:  ResBlock(B,E)\n",
    "      Out:\n",
    "        Conv2d(B→1, 3×3,1,1)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=3, base=96, emb=384, out_ch=1, dropout=0.1, use_attn=True):\n",
    "        super().__init__()\n",
    "        # ====== Implement here ======\n",
    "        # [cite_start]t-MLP: Linear(E->2E) -> SiLU -> Linear(2E->E) [cite: 51]\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(emb, emb * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb * 2, emb)\n",
    "        )\n",
    "        self.emb_dim = emb\n",
    "\n",
    "        # Down Path\n",
    "        # [cite_start]inp: Conv2d(in_ch->B, 3x3, 1, 1) [cite: 52]\n",
    "        self.conv_in = nn.Conv2d(in_ch, base, 3, 1, 1)\n",
    "        \n",
    "        # rb0: ResBlock(B, E)\n",
    "        self.rb0 = ResBlock(base, emb, dropout)\n",
    "        \n",
    "        # d1: Conv2d(B->2B, 4x4, 2, 1)\n",
    "        self.down1 = nn.Conv2d(base, base*2, 4, 2, 1)\n",
    "        \n",
    "        # rb1: ResBlock(2B, E)\n",
    "        self.rb1 = ResBlock(base*2, emb, dropout)\n",
    "        \n",
    "        # d2: Conv2d(2B->4B, 4x4, 2, 1)\n",
    "        self.down2 = nn.Conv2d(base*2, base*4, 4, 2, 1)\n",
    "        \n",
    "        # rb2: ResBlock(4B, E)\n",
    "        self.rb2 = ResBlock(base*4, emb, dropout)\n",
    "        \n",
    "        # [cite_start]Mid Path (8x8) [cite: 53]\n",
    "        self.mid1 = ResBlock(base*4, emb, dropout)\n",
    "        self.attn = SelfAttention2d(base*4) if use_attn else nn.Identity()\n",
    "        self.mid2 = ResBlock(base*4, emb, dropout)\n",
    "        \n",
    "        # [cite_start]Up Path [cite: 54, 55]\n",
    "        # u1: ConvTranspose2d(4B->2B, 4x4, 2, 1)\n",
    "        self.up1 = nn.ConvTranspose2d(base*4, base*2, 4, 2, 1)\n",
    "        \n",
    "        # red1: Conv2d(2B+2B->2B, 1x1) (Reduction after concat)\n",
    "        self.red1 = nn.Conv2d(base*2 + base*2, base*2, 1)\n",
    "        \n",
    "        # rb3: ResBlock(2B, E)\n",
    "        self.rb3 = ResBlock(base*2, emb, dropout)\n",
    "        \n",
    "        # u2: ConvTranspose2d(2B->B, 4x4, 2, 1)\n",
    "        self.up2 = nn.ConvTranspose2d(base*2, base, 4, 2, 1)\n",
    "        \n",
    "        # red2: Conv2d(B+B->B, 1x1) (Reduction after concat)\n",
    "        self.red2 = nn.Conv2d(base + base, base, 1)\n",
    "        \n",
    "        # rb4: ResBlock(B, E)\n",
    "        self.rb4 = ResBlock(base, emb, dropout)\n",
    "        \n",
    "        # [cite_start]Out: Conv2d(B->1, 3x3, 1, 1) [cite: 56]\n",
    "        self.conv_out = nn.Conv2d(base, out_ch, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        # ====== Implement here ======\n",
    "        # Embed time\n",
    "        # t is (B,) long/float\n",
    "        t_emb = sinusoidal_embed(t, self.emb_dim) # (B, emb)\n",
    "        emb = self.time_mlp(t_emb) # (B, emb)\n",
    "        \n",
    "        # --- Down ---\n",
    "        h_start = self.conv_in(x) # (B, base, 32, 32)\n",
    "        h_rb0 = self.rb0(h_start, emb) # (B, base, 32, 32)\n",
    "        \n",
    "        h_d1 = self.down1(h_rb0) # (B, 2*base, 16, 16)\n",
    "        h_rb1 = self.rb1(h_d1, emb)\n",
    "        \n",
    "        h_d2 = self.down2(h_rb1) # (B, 4*base, 8, 8)\n",
    "        h_rb2 = self.rb2(h_d2, emb)\n",
    "        \n",
    "        # --- Mid ---\n",
    "        h = self.mid1(h_rb2, emb)\n",
    "        h = self.attn(h)\n",
    "        h = self.mid2(h, emb)\n",
    "        \n",
    "        # --- Up ---\n",
    "        # Up 1\n",
    "        h = self.up1(h) # (B, 2*base, 16, 16)\n",
    "        # Skip connection from h_rb1 (size 16x16, 2*base)\n",
    "        h = torch.cat([h, h_rb1], dim=1) # (B, 4*base, 16, 16)\n",
    "        h = self.red1(h) # (B, 2*base, 16, 16)\n",
    "        h = self.rb3(h, emb)\n",
    "        \n",
    "        # Up 2\n",
    "        h = self.up2(h) # (B, base, 32, 32)\n",
    "        # Skip connection from h_rb0 (size 32x32, base)\n",
    "        h = torch.cat([h, h_rb0], dim=1) # (B, 2*base, 32, 32)\n",
    "        h = self.red2(h) # (B, base, 32, 32)\n",
    "        h = self.rb4(h, emb)\n",
    "        \n",
    "        # --- Out ---\n",
    "        out = self.conv_out(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Diffusion core (buffers + forward noising)\n",
    "# ----------------------------\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DCfg:\n",
    "    steps:int=400\n",
    "    beta_start:float=1e-4\n",
    "    beta_end:float=2e-2\n",
    "    beta_schedule:str=\"cosine\"  # {'linear','cosine'}\n",
    "\n",
    "def cosine_betas(T: int, s: float = 0.008):\n",
    "    r\"\"\"\n",
    "    ### TODO: implement cosine β_t from ᾱ(t)\n",
    "\n",
    "    ᾱ(t) = cos^2( ((t/T + s)/(1+s)) * π/2 ), for t in {0,...,T}\n",
    "    Then:\n",
    "      β_t = min(0.999, 1 - ᾱ(t+1)/ᾱ(t))   for t = 0..T-1\n",
    "    Return float32 tensor shape (T,)\n",
    "    \"\"\"\n",
    "    # ====== Implement here ======\n",
    "    # Create time grid from 0 to T\n",
    "    # We need t/T from 0 to 1.\n",
    "    steps = T + 1\n",
    "    x = torch.linspace(0, T, steps)\n",
    "    \n",
    "    # alphas_cumprod = cos^2( ((t/T + s) / (1 + s)) * pi/2 )\n",
    "    alphas_cumprod = torch.cos(((x / T) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    \n",
    "    # Scale so that alpha_bar_0 = 1\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    \n",
    "    # beta_t = 1 - (alpha_bar_t / alpha_bar_{t-1})\n",
    "    # Note: The formula corresponds to alpha_bar(t+1) / alpha_bar(t) in the loop\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    \n",
    "    # Clip max beta to 0.999\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, cfg: DCfg):\n",
    "        r\"\"\"\n",
    "        ### TODO: build buffers from β_t\n",
    "          α_t       = 1 - β_t\n",
    "          ᾱ_t      = ∏_{s=1}^t α_s\n",
    "          sqrt_ab   = √(ᾱ_t)\n",
    "          sqrt_1mab = √(1 - ᾱ_t)\n",
    "          sqrt_ra   = √(1/α_t)\n",
    "          post_var  = β_t * (1 - ᾱ_{t-1}) / (1 - ᾱ_t)   with ᾱ_0 = 1\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        T = cfg.steps\n",
    "        self.T = T\n",
    "        # ====== Implement here ======\n",
    "        # 1. Define Betas\n",
    "        if cfg.beta_schedule == \"cosine\":\n",
    "            betas = cosine_betas(T)\n",
    "        else:\n",
    "            # Linear schedule\n",
    "            betas = torch.linspace(cfg.beta_start, cfg.beta_end, T)\n",
    "            \n",
    "        # 2. Compute Alphas\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # 3. Register buffers (helper to register tensor attributes)\n",
    "        # Using simple assignment here, to_() will handle device move\n",
    "        self.betas = betas\n",
    "        self.alphas = alphas\n",
    "        self.a_bar = alphas_cumprod # This is alpha_bar\n",
    "        self.a_bar_prev = alphas_cumprod_prev\n",
    "        \n",
    "        # [cite_start]Pre-compute square roots for q_sample [cite: 26]\n",
    "        self.sqrt_ab = torch.sqrt(alphas_cumprod)\n",
    "        self.sqrt_1mab = torch.sqrt(1. - alphas_cumprod)\n",
    "        \n",
    "        # Pre-compute other helpers for posterior/sampling\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "        \n",
    "        # Posterior variance (used in DDPM, helpful reference)\n",
    "        # var = beta * (1 - alpha_bar_prev) / (1 - alpha_bar)\n",
    "        self.post_var = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "    def to_(self, d: torch.device):\n",
    "        for k,v in vars(self).items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                setattr(self, k, v.to(d))\n",
    "        return self\n",
    "\n",
    "    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: torch.Tensor=None):\n",
    "        r\"\"\"\n",
    "        ### TODO: forward noising\n",
    "          x_t = √(ᾱ_t) * x0 + √(1 - ᾱ_t) * ε,   ε~N(0,I)\n",
    "        Return (x_t, ε). Use provided ε if not None.\n",
    "        \"\"\"\n",
    "        # ====== Implement here ======\n",
    "        if eps is None:\n",
    "            eps = torch.randn_like(x0)\n",
    "            \n",
    "        # Extract coefs at timestep t\n",
    "        # shape broadcast: (B,) -> (B, 1, 1, 1)\n",
    "        sqrt_ab_t = self.sqrt_ab[t].view(-1, 1, 1, 1)\n",
    "        sqrt_1mab_t = self.sqrt_1mab[t].view(-1, 1, 1, 1)\n",
    "        \n",
    "        return sqrt_ab_t * x0 + sqrt_1mab_t * eps, eps\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# EMA (GIVEN)\n",
    "# ----------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k,v in model.state_dict().items()}\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        for k, v in model.state_dict().items():\n",
    "            if v.dtype.is_floating_point:\n",
    "                self.shadow[k].mul_(self.decay).add_(v, alpha=1.0 - self.decay)\n",
    "            else:\n",
    "                self.shadow[k] = v.detach().clone()\n",
    "    def copy_to(self, model: nn.Module):\n",
    "        model.load_state_dict(self.shadow, strict=True)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Pred space helpers & DDIM step\n",
    "# ----------------------------\n",
    "def to_eps_from_pred(pred_type, out, xt, sqrt_ab_t, sqrt_1mab_t):\n",
    "    r\"\"\"\n",
    "    ### TODO: map network output 'out' to (ε̂, x̂0) depending on prediction space.\n",
    "\n",
    "      If pred_type == 'eps':\n",
    "        ε̂   = out\n",
    "        x̂0  = (xt - √(1-ᾱ_t)*ε̂)/√(ᾱ_t)\n",
    "\n",
    "      If pred_type == 'x0':\n",
    "        x̂0  = out\n",
    "        ε̂   = (xt - √(ᾱ_t)*x̂0)/√(1-ᾱ_t)\n",
    "\n",
    "      If pred_type == 'v':\n",
    "        x̂0  = √(ᾱ_t)*xt - √(1-ᾱ_t)*out\n",
    "        ε̂   = √(1-ᾱ_t)*xt + √(ᾱ_t)*out\n",
    "    \"\"\"\n",
    "    # ====== Implement here ======\n",
    "    if pred_type == 'eps':\n",
    "        # out is epsilon\n",
    "        eps_hat = out\n",
    "        # x0 = (xt - sqrt(1-ab)*eps) / sqrt(ab)\n",
    "        x0_hat = (xt - sqrt_1mab_t * eps_hat) / sqrt_ab_t\n",
    "        \n",
    "    elif pred_type == 'x0':\n",
    "        # out is x0\n",
    "        x0_hat = out\n",
    "        # eps = (xt - sqrt(ab)*x0) / sqrt(1-ab)\n",
    "        eps_hat = (xt - sqrt_ab_t * x0_hat) / sqrt_1mab_t\n",
    "        \n",
    "    elif pred_type == 'v':\n",
    "        # [cite_start]out is v [cite: 32]\n",
    "        # x0 = sqrt(ab)*xt - sqrt(1-ab)*v\n",
    "        x0_hat = sqrt_ab_t * xt - sqrt_1mab_t * out\n",
    "        # eps = sqrt(1-ab)*xt + sqrt(ab)*v\n",
    "        eps_hat = sqrt_1mab_t * xt + sqrt_ab_t * out\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pred_type {pred_type}\")\n",
    "        \n",
    "    return eps_hat, x0_hat\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ddim_p_step(net, dd, x_t, m, y, t, t_prev, pred_type=\"v\", self_cond=False, x0_sc=None, eta=0.0, coord_conv=False):\n",
    "    r\"\"\"\n",
    "    ### DDIM deterministic step (η=0) with optional noise (η>0):\n",
    "      ā_t = ᾱ_t, ā_{t'} = ᾱ_{t_prev} [or 1 if t_prev=None]\n",
    "      σ_t = η * sqrt( (1 - ā_{t'})/(1 - ā_t) * (1 - ā_t / ā_{t'}) )\n",
    "      x_{t'} = √(ā_{t'}) * x̂0 + √(1 - ā_{t'} - σ_t^2) * ε̂  + 1_{η>0} * σ_t * z\n",
    "\n",
    "    We use DDIM (η=0) because it is deterministic and ensures consistency\n",
    "    across different predictions. The deterministic nature also allows for\n",
    "    much less number of steps during inference which makes it faster.\n",
    "    \n",
    "    Inputs include conditioning channels [x_t, m, y, (x0_sc), (coords?)].\n",
    "    Return (x_{t'}, x̂0).\n",
    "    \"\"\"\n",
    "    # ====== Implement here ======\n",
    "    B, _, H, W = x_t.shape\n",
    "    \n",
    "    # 1. Prepare Model Input\n",
    "    # [cite_start][x_t, m, y, (x0_sc), (coords?)] [cite: 48]\n",
    "    din = [x_t, m, y]\n",
    "    if self_cond:\n",
    "        din.append(x0_sc if x0_sc is not None else torch.zeros_like(x_t))\n",
    "        \n",
    "    if coord_conv:\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.linspace(-1, 1, H, device=x_t.device),\n",
    "            torch.linspace(-1, 1, W, device=x_t.device),\n",
    "            indexing=\"ij\"\n",
    "        )\n",
    "        coords = torch.stack([xx, yy], dim=0).expand(B, -1, -1, -1)\n",
    "        din.append(coords)\n",
    "        \n",
    "    # 2. Predict\n",
    "    inp = torch.cat(din, dim=1)\n",
    "    out = net(inp, t)\n",
    "    \n",
    "    # [cite_start]3. Map to eps/x0 [cite: 67]\n",
    "    # Extract alpha_bar for current t\n",
    "    sqrt_ab_t = dd.sqrt_ab[t].view(-1, 1, 1, 1)\n",
    "    sqrt_1mab_t = dd.sqrt_1mab[t].view(-1, 1, 1, 1)\n",
    "    \n",
    "    eps_hat, x0_hat = to_eps_from_pred(pred_type, out, x_t, sqrt_ab_t, sqrt_1mab_t)\n",
    "    \n",
    "    # 4. DDIM Step\n",
    "    # Handle t_prev (if None, implies t_prev < 0, so alpha_bar_prev = 1.0)\n",
    "    if t_prev is None:\n",
    "        ab_prev = torch.ones_like(dd.a_bar[t])\n",
    "    else:\n",
    "        ab_prev = dd.a_bar[t_prev]\n",
    "        \n",
    "    ab_cur = dd.a_bar[t]\n",
    "    \n",
    "    # View for broadcasting\n",
    "    ab_prev = ab_prev.view(-1, 1, 1, 1)\n",
    "    ab_cur = ab_cur.view(-1, 1, 1, 1)\n",
    "    \n",
    "    # [cite_start]Sigma (std dev) [cite: 35]\n",
    "    # sigma = eta * sqrt( (1 - ab_prev)/(1 - ab_cur) * (1 - ab_cur/ab_prev) )\n",
    "    sigma_t = eta * torch.sqrt(\n",
    "        (1 - ab_prev) / (1 - ab_cur) * (1 - ab_cur / ab_prev)\n",
    "    )\n",
    "    \n",
    "    # Direction pointing to x_t\n",
    "    # dir = sqrt(1 - ab_prev - sigma^2) * eps_hat\n",
    "    pred_dir = torch.sqrt((1 - ab_prev - sigma_t**2).clamp(min=0.0)) * eps_hat\n",
    "    \n",
    "    # Random noise (only if eta > 0)\n",
    "    if eta > 0:\n",
    "        noise = torch.randn_like(x_t)\n",
    "        sigma_z = sigma_t * noise\n",
    "    else:\n",
    "        sigma_z = 0.0\n",
    "        \n",
    "    # [cite_start]Final reconstruction x_{t'} [cite: 36]\n",
    "    x_prev = torch.sqrt(ab_prev) * x0_hat + pred_dir + sigma_z\n",
    "    \n",
    "    return x_prev, x0_hat\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def enforce_known(dd, x_t, y, m, t, z_fixed=None):\n",
    "    r\"\"\"\n",
    "    Per-step data-consistency on known pixels (m=1):\n",
    "      y_t = √(ᾱ_t) * y + √(1-ᾱ_t) * z_t\n",
    "      x_t ← m ⊙ y_t  +  (1-m) ⊙ x_t\n",
    "    If z_fixed is provided, reuse it; otherwise use zeros for stability.\n",
    "    \"\"\"\n",
    "    z = torch.zeros_like(y) if z_fixed is None else z_fixed\n",
    "    y_t = dd.sqrt_ab[t].view(-1,1,1,1) * y + dd.sqrt_1mab[t].view(-1,1,1,1) * z\n",
    "    return m * y_t + (1.0 - m) * x_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55b3ba",
   "metadata": {},
   "source": [
    "## 6. Loss & Inference _(GIVEN — do not modify)_\n",
    "\n",
    "Implements training loss and the DDIM inpainting sampler used at test time.\n",
    "\n",
    "- **`p2_weight(a_bar_t, k, gamma)`** — SNR-based P2 reweighting for timestep-balanced training.\n",
    "- **`loss_fn(net, dd, x0, m, y, t, ...)`** — Trains in chosen prediction space (`eps` / `x0` / `v`), uses `dd.q_sample` to form `(x_t, ε)`, optional **self-conditioning** and **coord-conv**, pixelwise **L2** with **hole upweighting** via `hole_weight`, and multiplies by **P2** weights; returns the mean loss.\n",
    "- **`inpaint(net, dd, y, m, ...)`** — DDIM inpainting loop over a timestep grid; supports deterministic (`η=0`) or noisy (`η>0`) updates, optional `init_from_y`, per-step **data consistency** via `enforce_known` repeated `dc_repeats` times, optional fixed noise for DC, and optional self-conditioning across steps.\n",
    "\n",
    "> **Heads-up:** Keep the sampler `pred_type` consistent with training. Lower `steps` if memory is tight; `dc_repeats=2` often improves seam quality; use `η=0` for classic DDIM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859289d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2_weight(a_bar_t, k=1.0, gamma=1.0):\n",
    "    snr = a_bar_t / (1.0 - a_bar_t + 1e-8)\n",
    "    return torch.pow(k + snr, -gamma)\n",
    "\n",
    "def loss_fn(net, dd, x0, m, y, t, pred_type=\"v\", hole_weight=5.0, p2_k=1.0, p2_gamma=1.0,\n",
    "            self_cond=False, coord_conv=False):\n",
    "    \"\"\"\n",
    "    Training loss (noise prediction in chosen space) with hole upweight + p2.\n",
    "    \"\"\"\n",
    "    xt, eps = dd.q_sample(x0, t)\n",
    "    B, _, H, W = x0.shape\n",
    "\n",
    "    # coord-conv channels\n",
    "    coords = None\n",
    "    if coord_conv:\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.linspace(-1, 1, H, device=x0.device),\n",
    "            torch.linspace(-1, 1, W, device=x0.device),\n",
    "            indexing=\"ij\"\n",
    "        )\n",
    "        coords = torch.stack([xx, yy], dim=0).expand(B, -1, -1, -1)  # (B,2,H,W)\n",
    "\n",
    "    # self-conditioning (50%): zero-SC prepass to get x0_sc\n",
    "    # When U-Net tries to predict clean image from a noisy one,\n",
    "    # it is making a blind guess every time. We solve this\n",
    "    # problem by passing its own previous prediction as an extra\n",
    "    # input 50% of the time. This lets U-Net refine its own work,\n",
    "    # leading to more coherent images.\n",
    "    if self_cond and (random.random() < 0.5):\n",
    "        din0 = [xt, m, y, torch.zeros_like(x0)]\n",
    "        if coords is not None: din0.append(coords)\n",
    "        out0 = net(torch.cat(din0, dim=1), t)\n",
    "        sqrt_ab_t   = dd.sqrt_ab[t].view(-1,1,1,1)\n",
    "        sqrt_1mab_t = dd.sqrt_1mab[t].view(-1,1,1,1)\n",
    "        _, x0_hat0 = to_eps_from_pred(pred_type, out0, xt, sqrt_ab_t, sqrt_1mab_t)\n",
    "        sc = x0_hat0.detach()\n",
    "        din = [xt, m, y, sc]\n",
    "    else:\n",
    "        din = [xt, m, y] + ([torch.zeros_like(x0)] if self_cond else [])\n",
    "\n",
    "    if coords is not None:\n",
    "        din.append(coords)\n",
    "\n",
    "    out = net(torch.cat(din, dim=1), t)\n",
    "\n",
    "    # target in chosen pred space\n",
    "    if pred_type == \"eps\":\n",
    "        target = eps\n",
    "    elif pred_type == \"x0\":\n",
    "        target = x0\n",
    "    else:  # 'v'\n",
    "        target = dd.sqrt_ab[t].view(-1,1,1,1) * eps - dd.sqrt_1mab[t].view(-1,1,1,1) * x0\n",
    "\n",
    "    # pixelwise L2 with hole upweight + p2 weighting\n",
    "    # We use Perception Prioritized Weighting (P2 weighting) because\n",
    "    # it forces model to learn shapes and content instead of \n",
    "    # indistinguishable noise. We do so by assigning higher weights\n",
    "    # in the initial steps of denoising where the SNR is low\n",
    "    # (Generating the shape from noise) and lower weights in the\n",
    "    # later steps where SNR is high (fixing minor noise).\n",
    "    per_pix = (out - target)**2\n",
    "    if hole_weight and hole_weight != 1.0:\n",
    "        weight_mask = 1.0 + (hole_weight - 1.0) * (1.0 - m)\n",
    "        per_pix = per_pix * weight_mask\n",
    "\n",
    "    w = p2_weight(dd.a_bar[t], k=p2_k, gamma=p2_gamma).view(-1,1,1,1)\n",
    "    return (w * per_pix).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inpaint(net, dd, y, m, steps=50, d=None, dc_repeats=2, dc_fixed_z=True,\n",
    "            pred_type=\"v\", self_cond=False, init_from_y=True,\n",
    "            eta=0.0, coord_conv=False):\n",
    "    device = y.device\n",
    "    tau = torch.linspace(0, dd.T - 1, steps, device=device).round().long()\n",
    "    B = y.size(0)\n",
    "    if init_from_y:\n",
    "        x = m * y + (1.0 - m) * torch.randn_like(y)\n",
    "    else:\n",
    "        x = torch.randn_like(y)\n",
    "    z0 = torch.randn_like(y) if dc_fixed_z else None\n",
    "    x0_sc = None\n",
    "\n",
    "    for i in reversed(range(len(tau))):\n",
    "        ti = tau[i]; tb = ti.repeat(B)\n",
    "        for _ in range(max(1, dc_repeats)):\n",
    "            x = enforce_known(dd, x, y, m, tb, z_fixed=z0)\n",
    "\n",
    "        t_prev = tau[i-1] if i > 0 else None\n",
    "        x, x0_sc = ddim_p_step(net, dd, x, m, y, tb, t_prev, pred_type=pred_type,\n",
    "                               self_cond=self_cond, x0_sc=x0_sc, eta=eta, coord_conv=coord_conv)\n",
    "    x = m * y + (1.0 - m) * x\n",
    "    return x.clamp(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0bb0fb",
   "metadata": {},
   "source": [
    "## 7. Train / Sample / Eval helpers _(GIVEN — no argparse)_\n",
    "\n",
    "High-level helpers to **train**, **sample**, and **evaluate** without CLI flags.\n",
    "\n",
    "- **`train(...)`** — Runs full training with AdamW, LR **warmup**, **EMA** tracking, optional **grad accumulation** & **grad clipping**. Periodically saves **inpainting panels** (`outputs/inpaint/panel_*.png`) using the EMA copy, and finally writes a checkpoint to `outputs/inpaint/last.pt` (with config).\n",
    "- **`sample_cmd(...)`** — Loads the checkpoint (prefers **EMA** weights), rebuilds diffusion and UNet from saved config, inpaints a test batch, and saves `outputs/inpaint/samples.png`.\n",
    "- **`eval_cmd(...)`** — Loads the checkpoint (prefers **EMA**), inpaints the MNIST test set up to `n_eval` images, computes **hole-only PSNR/L1**, and writes `results/inpaint_metrics.json`.\n",
    "\n",
    "> **Heads-up:**  \n",
    "> • Keep `pred` consistent between training and sampling.  \n",
    "> • If memory is tight, lower `batch_size`, raise `grad_accum`, or reduce `steps/sample_steps`.  \n",
    "> • Outputs are saved under `outputs/inpaint/` and metrics under `results/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=3, batch_size=128, lr=2e-4, steps=400,\n",
    "          beta_start=1e-4, beta_end=2e-2, beta_schedule=\"cosine\",\n",
    "          base=96, emb=384, sample_every=400, sample_steps=50,\n",
    "          center_box=12, dc_repeats=2, dc_fixed_z=True,\n",
    "          pred=\"v\", p2_k=1.0, p2_gamma=1.0, hole_weight=5.0,\n",
    "          seed=42, clip_grad=1.0, ema_decay=0.999, warmup_steps=1000,\n",
    "          self_cond=True, eta=0.0, coord_conv=True,\n",
    "          grad_accum:int=1):\n",
    "    seed_all(seed); d = dev()\n",
    "    train_loader, test_loader, (C,H,W) = get_mnist_loader(batch_size=batch_size, num_workers=2)\n",
    "\n",
    "    in_extra = (1 if self_cond else 0) + (2 if coord_conv else 0)\n",
    "    in_ch = 3 + in_extra\n",
    "    net = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1, dropout=0.1, use_attn=True).to(d)\n",
    "\n",
    "    dd = Diffusion(DCfg(steps=steps, beta_start=beta_start, beta_end=beta_end,\n",
    "                        beta_schedule=beta_schedule)).to_(d)\n",
    "\n",
    "    opt = torch.optim.AdamW(net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    ema = EMA(net, decay=ema_decay)\n",
    "\n",
    "    step_i = 0\n",
    "    scaler = 1.0 / max(1, grad_accum)\n",
    "    for ep in range(epochs):\n",
    "        running = 0.0\n",
    "        for it,(x,_) in enumerate(tqdm(train_loader, desc=f\"Epoch {ep+1}/{epochs}\")):\n",
    "            x = x.to(d)\n",
    "            B = x.size(0)\n",
    "            m = make_center_box_mask(B,H,W, box=center_box).to(d)\n",
    "            y = m * x\n",
    "            t = torch.randint(0, steps, (B,), device=d, dtype=torch.long)\n",
    "\n",
    "            if warmup_steps and step_i < warmup_steps:\n",
    "                warm_lr = lr * float(step_i + 1) / float(warmup_steps)\n",
    "                for pg in opt.param_groups: pg[\"lr\"] = warm_lr\n",
    "\n",
    "            loss = loss_fn(net, dd, x, m, y, t,\n",
    "                           pred_type=pred, hole_weight=hole_weight,\n",
    "                           p2_k=p2_k, p2_gamma=p2_gamma, self_cond=self_cond,\n",
    "                           coord_conv=coord_conv)\n",
    "            (loss * scaler).backward()\n",
    "            running += float(loss)\n",
    "\n",
    "            if ((it + 1) % grad_accum) == 0:\n",
    "                if clip_grad and clip_grad > 0:\n",
    "                    nn.utils.clip_grad_norm_(net.parameters(), max_norm=clip_grad)\n",
    "                opt.step(); opt.zero_grad(); ema.update(net)\n",
    "                step_i += 1\n",
    "\n",
    "                if (step_i % sample_every) == 0:\n",
    "                    with torch.no_grad():\n",
    "                        net_ema = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1).to(d)\n",
    "                        ema.copy_to(net_ema); net_ema.eval()\n",
    "                        idx = slice(0, min(16, B))\n",
    "                        comp = inpaint(net_ema, dd, y[idx], m[idx], steps=sample_steps, d=d,\n",
    "                                       dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
    "                                       pred_type=pred, self_cond=self_cond, init_from_y=True,\n",
    "                                       eta=eta, coord_conv=coord_conv)\n",
    "                        panel = torch.cat([x[idx], y[idx], comp], dim=0)\n",
    "                        save_grid(panel, f\"outputs/inpaint/panel_{step_i:06d}.png\", nrow=panel.size(0)//3)\n",
    "                        del net_ema\n",
    "\n",
    "        print(f\"[Epoch {ep+1}] mean loss: {running / max(1,len(train_loader)):.4f}\")\n",
    "\n",
    "    ensure_dir(\"outputs/inpaint\")\n",
    "    torch.save({\n",
    "        \"net\": net.state_dict(),\n",
    "        \"ema\": ema.shadow,\n",
    "        \"cfg\": dict(epochs=epochs, batch_size=batch_size, lr=lr, steps=steps,\n",
    "                    beta_start=beta_start, beta_end=beta_end, beta_schedule=beta_schedule,\n",
    "                    base=base, emb=emb, sample_every=sample_every, sample_steps=sample_steps,\n",
    "                    center_box=center_box, dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
    "                    pred=pred, p2_k=p2_k, p2_gamma=p2_gamma, hole_weight=hole_weight,\n",
    "                    seed=seed, clip_grad=clip_grad, ema_decay=ema_decay, warmup_steps=warmup_steps,\n",
    "                    self_cond=int(self_cond), eta=eta, coord_conv=int(coord_conv),\n",
    "                    grad_accum=grad_accum)},\n",
    "               \"outputs/inpaint/last.pt\")\n",
    "\n",
    "    # Final panel with EMA\n",
    "    with torch.no_grad():\n",
    "        _, test_loader, _ = get_mnist_loader(batch_size=16, num_workers=0)\n",
    "        x,_ = next(iter(test_loader))\n",
    "        x = x.to(d); B = min(16, x.size(0))\n",
    "        m = make_center_box_mask(B,32,32, box=center_box).to(d)\n",
    "        y = m * x[:B]\n",
    "        net_ema = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1).to(d)\n",
    "        pkg = safe_torch_load(\"outputs/inpaint/last.pt\", map_location=\"cpu\")\n",
    "        state = pkg.get(\"ema\", pkg.get(\"net\"))\n",
    "        net_ema.load_state_dict(state, strict=False)\n",
    "        net_ema.eval()\n",
    "        comp = inpaint(net_ema, dd, y, m, steps=sample_steps, d=d,\n",
    "                       dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
    "                       pred_type=pred, self_cond=self_cond, init_from_y=True,\n",
    "                       eta=eta, coord_conv=coord_conv)\n",
    "        panel = torch.cat([x[:B], y, comp], dim=0)\n",
    "        save_grid(panel, \"outputs/inpaint/panel_final.png\", nrow=B)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_cmd(ckpt=\"outputs/inpaint/last.pt\", n=16, steps=50,\n",
    "               center_box=12, dc_repeats=2, dc_fixed_z=True,\n",
    "               pred=\"v\", self_cond=True, init_from_y=True,\n",
    "               eta=0.0, coord_conv=True):\n",
    "    d = dev()\n",
    "    pkg = safe_torch_load(ckpt, map_location=\"cpu\")\n",
    "    cfg = pkg.get(\"cfg\", {})\n",
    "    base = cfg.get(\"base\", 96); emb = cfg.get(\"emb\", 384)\n",
    "    dd = Diffusion(DCfg(steps=cfg.get(\"steps\", steps),\n",
    "                        beta_start=cfg.get(\"beta_start\", 1e-4),\n",
    "                        beta_end=cfg.get(\"beta_end\", 2e-2),\n",
    "                        beta_schedule=cfg.get(\"beta_schedule\",\"cosine\"))).to_(d)\n",
    "    in_extra = (1 if int(cfg.get(\"self_cond\", 1)) else 0) + (2 if int(cfg.get(\"coord_conv\", 1)) else 0)\n",
    "    in_ch = 3 + in_extra\n",
    "    net = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1).to(d)\n",
    "    state = pkg.get(\"ema\", pkg.get(\"net\"))\n",
    "    net.load_state_dict(state, strict=False); net.eval()\n",
    "\n",
    "    _, test_loader, (C,H,W) = get_mnist_loader(batch_size=n, num_workers=0)\n",
    "    x,_ = next(iter(test_loader))\n",
    "    x = x.to(d); B = min(n, x.size(0))\n",
    "    m = make_center_box_mask(B,H,W, box=center_box).to(d)\n",
    "    y = m * x[:B]\n",
    "    comp = inpaint(net, dd, y, m, steps=steps, d=d,\n",
    "                   dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
    "                   pred_type=pred, self_cond=bool(int(cfg.get(\"self_cond\",1))),\n",
    "                   init_from_y=init_from_y, eta=float(cfg.get(\"eta\", 0.0)),\n",
    "                   coord_conv=bool(int(cfg.get(\"coord_conv\",1))))\n",
    "    panel = torch.cat([x[:B], y, comp], dim=0)\n",
    "    save_grid(panel, \"outputs/inpaint/samples.png\", nrow=B)\n",
    "    print(\"Wrote outputs/inpaint/samples.png\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_cmd(ckpt=\"outputs/inpaint/last.pt\", batch_size=256, n_eval=500, steps=50,\n",
    "             center_box=12, dc_repeats=2, dc_fixed_z=True, pred=\"v\",\n",
    "             self_cond=True, init_from_y=True, eta=0.0,\n",
    "             coord_conv=True, seed=123):\n",
    "    seed_all(seed); d = dev()\n",
    "    pkg = safe_torch_load(ckpt, map_location=\"cpu\")\n",
    "    cfg = pkg.get(\"cfg\", {})\n",
    "    base = cfg.get(\"base\", 96); emb = cfg.get(\"emb\", 384)\n",
    "    dd = Diffusion(DCfg(steps=cfg.get(\"steps\", steps),\n",
    "                        beta_start=cfg.get(\"beta_start\", 1e-4),\n",
    "                        beta_end=cfg.get(\"beta_end\", 2e-2),\n",
    "                        beta_schedule=cfg.get(\"beta_schedule\",\"cosine\"))).to_(d)\n",
    "    in_extra = (1 if int(cfg.get(\"self_cond\", 1)) else 0) + (2 if int(cfg.get(\"coord_conv\", 1)) else 0)\n",
    "    in_ch = 3 + in_extra\n",
    "    net = UNetDeep(in_ch=in_ch, base=base, emb=emb, out_ch=1).to(d)\n",
    "    state = pkg.get(\"ema\", pkg.get(\"net\"))\n",
    "    net.load_state_dict(state, strict=False); net.eval()\n",
    "\n",
    "    _, test_loader, (C,H,W) = get_mnist_loader(batch_size=batch_size, num_workers=2)\n",
    "    tot_psnr, tot_l1, tot_n = 0.0, 0.0, 0\n",
    "    for x,_ in test_loader:\n",
    "        x = x.to(d); B = x.size(0)\n",
    "        m = make_center_box_mask(B,H,W, box=center_box).to(d)\n",
    "        y = m * x\n",
    "        comp = inpaint(net, dd, y, m, steps=steps, d=d,\n",
    "                       dc_repeats=dc_repeats, dc_fixed_z=dc_fixed_z,\n",
    "                       pred_type=pred, self_cond=bool(int(cfg.get(\"self_cond\",1))),\n",
    "                       init_from_y=init_from_y, eta=float(cfg.get(\"eta\", 0.0)),\n",
    "                       coord_conv=bool(int(cfg.get(\"coord_conv\",1))))\n",
    "        tot_psnr += psnr_on_mask(comp, x, m) * B\n",
    "        tot_l1   += l1_on_mask(comp, x, m) * B\n",
    "        tot_n    += B\n",
    "        if tot_n >= n_eval: break\n",
    "    avg_psnr = float(tot_psnr / max(1,tot_n))\n",
    "    avg_l1   = float(tot_l1 / max(1,tot_n))\n",
    "    ensure_dir(\"results\")\n",
    "    with open(\"results/inpaint_metrics.json\", \"w\") as f:\n",
    "        json.dump({\"psnr_hole\": avg_psnr, \"l1_hole\": avg_l1}, f, indent=2)\n",
    "    print(json.dumps({\"psnr_hole\": avg_psnr, \"l1_hole\": avg_l1}, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed4a00",
   "metadata": {},
   "source": [
    "## 8. How to run (after you finish the TODOs)\n",
    "\n",
    "1. **Implement all TODOs** in **Section 5** (embedding, UNet, cosine betas, diffusion buffers/q_sample, pred mapping, DDIM step).\n",
    "2. **Train** for a few epochs (Colab-friendly config below).\n",
    "3. **Sample** to visualize results.\n",
    "4. **Evaluate** PSNR/L1 on the hole region.\n",
    "\n",
    "> 💡 Tip: Start small for quick iteration — e.g., `epochs=3`, `steps=200`, `sample_steps=25`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35537754",
   "metadata": {},
   "source": [
    "## 9. Train _(run after completing Section 5)_\n",
    "\n",
    "Runs AdamW with EMA and saves:\n",
    "- **Checkpoint:** `outputs/inpaint/last.pt`\n",
    "- **Panels (every `sample_every`):** `outputs/inpaint/panel_*.png`\n",
    "\n",
    "**Edit the config** to fit your GPU (lower `batch_size`, reduce `steps/sample_steps`, or increase `grad_accum` if you hit Out of Memory).  \n",
    "> Note: This cell will error until all **TODOs** in Section 5 are implemented. Keep `pred` consistent with later sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c777cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    epochs=25,\n",
    "    batch_size=256,\n",
    "    lr=2e-4,\n",
    "    steps=1000,\n",
    "    sample_every=400,\n",
    "    sample_steps=50,\n",
    "    center_box=12,\n",
    "    dc_repeats=2,\n",
    "    dc_fixed_z=True,\n",
    "    pred=\"v\",\n",
    "    p2_k=1.0,\n",
    "    p2_gamma=1.0,\n",
    "    hole_weight=5.0,\n",
    "    seed=42,\n",
    "    clip_grad=1.0,\n",
    "    ema_decay=0.999,\n",
    "    warmup_steps=1000,\n",
    "    self_cond=True,\n",
    "    eta=0.0,\n",
    "    coord_conv=True,\n",
    "    grad_accum=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6afe90",
   "metadata": {},
   "source": [
    "## 10. Sample _(after training)_\n",
    "\n",
    "Loads `outputs/inpaint/last.pt` (prefers **EMA** weights), rebuilds UNet/Diffusion from the saved config, inpaints a small test batch, and writes **`outputs/inpaint/samples.png`**.\n",
    "\n",
    "> Keep `pred` consistent with training. You can adjust `steps`, `center_box`, `eta`, or `init_from_y` for different looks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_cmd(\n",
    "    ckpt=\"outputs/inpaint/last.pt\",\n",
    "    n=16,\n",
    "    steps=50,\n",
    "    center_box=12,\n",
    "    dc_repeats=2,\n",
    "    dc_fixed_z=True,\n",
    "    pred=\"v\",\n",
    "    self_cond=True,\n",
    "    init_from_y=True,\n",
    "    eta=0.0,\n",
    "    coord_conv=True,\n",
    ")\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"outputs/inpaint/samples.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6b0f3",
   "metadata": {},
   "source": [
    "## 11. Evaluate _(hole-only PSNR/L1)_\n",
    "\n",
    "Runs inpainting on the MNIST **test** split up to `n_eval` images, computes **PSNR/L1 over the hole region**, and saves **`results/inpaint_metrics.json`** (also prints the values).\n",
    "\n",
    "> For fair comparison, match `steps` and `center_box` with training; use `η=0` for deterministic DDIM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31954a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cmd(\n",
    "    ckpt=\"outputs/inpaint/last.pt\",\n",
    "    batch_size=256,\n",
    "    n_eval=500,\n",
    "    steps=50,\n",
    "    center_box=12,\n",
    "    dc_repeats=2,\n",
    "    dc_fixed_z=True,\n",
    "    pred=\"v\",\n",
    "    self_cond=True,\n",
    "    init_from_y=True,\n",
    "    eta=0.0,\n",
    "    coord_conv=True,\n",
    "    seed=123,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
